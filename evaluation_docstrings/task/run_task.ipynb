{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf305e-a1c4-481d-8212-801432ddff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbba84-fbe8-4236-8d24-e5b57aaf30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from evaluation_docstrings.llm.utils import load_secrets\n",
    "from evaluation_docstrings.llm.config import costs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from evaluation_docstrings.evaluation.LLMEvals import LLMEvals\n",
    "from evaluation_docstrings.evaluation.ProgrammaticEvals import ProgrammaticEvals\n",
    "from evaluation_docstrings.task.utils import (\n",
    "    read_code_into_dict,\n",
    "    convert_code_dict_to_string,\n",
    "    format_result_as_string,\n",
    ")\n",
    "from evaluation_docstrings.plotting.utils import generate_basic_analysis_cost_latency\n",
    "from evaluation_docstrings.prompting.task_prompts import (\n",
    "    DocStringPrompt,\n",
    "    CodeImprovements,\n",
    ")\n",
    "from evaluation_docstrings.llm.structured_output_callers import (\n",
    "    AnthropicStructuredOutputCaller,\n",
    "    OpenAIStructuredOutputCaller,\n",
    "    GeminiStructuredOutputCaller,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b15213-c10d-472a-a69a-90713cc7899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = load_secrets()\n",
    "gemini_caller = GeminiStructuredOutputCaller(\n",
    "    api_key=secrets[\"GEMINI_API_KEY\"], model=\"gemini-1.5-flash\"\n",
    ")\n",
    "open_ai_caller = OpenAIStructuredOutputCaller(api_key=secrets[\"OPENAI_API_KEY\"])\n",
    "claude_caller = AnthropicStructuredOutputCaller(api_key=secrets[\"ANTHROPIC_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dee51-e7fa-44c3-a8ac-fb82ba4a2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_loop(llm, codes, model_name=None):\n",
    "    result = defaultdict(list)\n",
    "    for i, code_example in enumerate(codes):\n",
    "        print(f\"Processing code example {i} : {code_example}\")\n",
    "\n",
    "        code_class = code_example.split(\"/\")[-2]\n",
    "        code_dict = read_code_into_dict(code_example)\n",
    "        code_string = convert_code_dict_to_string(code_dict)\n",
    "\n",
    "        code_string = f\"\"\"\n",
    "        <input_code>\n",
    "        {code_string}\n",
    "        <input_code>\n",
    "        \"\"\"\n",
    "\n",
    "        t1 = time.time()\n",
    "        if model_name:\n",
    "            res = llm.invoke(\n",
    "                code_string,\n",
    "                DocStringPrompt,\n",
    "                CodeImprovements,\n",
    "                temperature=0,\n",
    "                input_model_name=model_name,\n",
    "            )\n",
    "        else:\n",
    "            res = llm.invoke(\n",
    "                code_string,\n",
    "                DocStringPrompt,\n",
    "                CodeImprovements,\n",
    "                temperature=0,\n",
    "            )\n",
    "\n",
    "        model_output = res[\"model_dump\"]\n",
    "        t2 = time.time()\n",
    "\n",
    "        is_suitable_code = model_output.get(\"suitable_code\", False)\n",
    "        if not is_suitable_code:\n",
    "            print(f\"{code_string} not found to be suitable for docstrings\")\n",
    "\n",
    "        tokens = llm.token_counter(res)\n",
    "        input_tokens = tokens[\"input_tokens\"]\n",
    "        output_tokens = tokens[\"output_tokens\"]\n",
    "\n",
    "        result[\"code_id\"].append(i)\n",
    "        result[\"code_type\"].append(code_class)\n",
    "        result[\"input_code\"].append(code_string)\n",
    "        result[\"model_output\"].append(model_output)\n",
    "        result[\"input_tokens\"].append(input_tokens)\n",
    "        result[\"output_tokens\"].append(output_tokens)\n",
    "        result[\"suitable_code\"].append(is_suitable_code)\n",
    "\n",
    "        if is_suitable_code:\n",
    "            result[\"number_of_docstrings\"].append(\n",
    "                len(model_output.get(\"docstrings\", []))\n",
    "            )\n",
    "        else:\n",
    "            result[\"number_of_docstrings\"].append(0)\n",
    "\n",
    "        if \"summary\" in model_output:\n",
    "            result[\"summary\"].append(model_output[\"summary\"])\n",
    "        else:\n",
    "            result[\"summary\"].append(None)\n",
    "        result[\"process_time\"].append(t2 - t1)\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df[\"code_path\"] = codes\n",
    "    result_df[\"docstrings\"] = result_df[\"model_output\"].apply(format_result_as_string)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acdec6-3e71-465f-8adb-637ea7d35fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(llm, df, ground_truth=None, **kwargs):\n",
    "    evaluation_result = defaultdict(list)\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        code_path = row[\"code_path\"]\n",
    "        code_id = row[\"code_id\"]\n",
    "        input_code = row[\"input_code\"]\n",
    "        model_output = eval(str(row[\"model_output\"]))\n",
    "\n",
    "        # programmatic tests\n",
    "        fields_correct = ProgrammaticEvals.check_correct_fields(model_output)\n",
    "        word_count_correct = ProgrammaticEvals.check_explanation_word_count(\n",
    "            model_output\n",
    "        )\n",
    "        line_numbers_match = ProgrammaticEvals.check_line_numbers_and_methods(\n",
    "            model_output, input_code\n",
    "        )\n",
    "\n",
    "        # LLM tests\n",
    "        t1 = time.time()\n",
    "        evaluation_response, tokens = LLMEvals.docstring_quality(\n",
    "            llm, input_code, model_output, **kwargs\n",
    "        )\n",
    "        t2 = time.time()\n",
    "        model_output = evaluation_response[\"model_dump\"]\n",
    "        input_tokens = tokens[\"input_tokens\"]\n",
    "        output_tokens = tokens[\"output_tokens\"]\n",
    "\n",
    "        evaluation_result[\"code_id\"].append(code_id)\n",
    "        evaluation_result[\"code_path\"].append(code_path)\n",
    "        evaluation_result[\"code_type\"].append(row[\"code_type\"])\n",
    "        evaluation_result[\"input_code\"].append(row[\"input_code\"])\n",
    "        evaluation_result[\"docstrings\"].append(row[\"docstrings\"])\n",
    "        evaluation_result[\"critique\"].append(model_output[\"critique\"])\n",
    "        evaluation_result[\"fields_correct\"].append(fields_correct)\n",
    "        evaluation_result[\"word_count_correct\"].append(word_count_correct)\n",
    "        evaluation_result[\"line_numbers_correct\"].append(line_numbers_match)\n",
    "        evaluation_result[\"accuracy_score\"].append(int(model_output[\"accuracy\"]))\n",
    "        evaluation_result[\"coverage_score\"].append(int(model_output[\"coverage\"]))\n",
    "        evaluation_result[\"clarity_score\"].append(int(model_output[\"clarity\"]))\n",
    "        evaluation_result[\"process_time\"].append(t2 - t1)\n",
    "        evaluation_result[\"input_tokens\"].append(input_tokens)\n",
    "        evaluation_result[\"output_tokens\"].append(output_tokens)\n",
    "        evaluation_df = pd.DataFrame(evaluation_result)\n",
    "\n",
    "    # join on ground truth labels if we have them\n",
    "    if isinstance(ground_truth, pd.DataFrame):\n",
    "        evaluation_df = evaluation_df.merge(ground_truth, on=\"code_id\")\n",
    "    evaluation_df[\"code_type\"] = evaluation_df[\"code_path\"].apply(\n",
    "        lambda x: x.split(\"/\")[-2]\n",
    "    )\n",
    "    evaluation_df[\"code_name\"] = evaluation_df[\"code_path\"].apply(\n",
    "        lambda x: x.split(\"/\")[-1]\n",
    "    )\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778fae5-3055-4395-80ec-94f5e253ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = glob.glob(\"../code_database/testing/leetcode/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93e115-7938-4b13-b5ac-f69dfb4e5198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt_4o_mini_result = generation_loop(\n",
    "    open_ai_caller, all_codes, model_name=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31056a1e-6b1f-4700-8216-1da022dd4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_result = generation_loop(gemini_caller, all_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e125-6672-48fb-8976-db62f3b3343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_result[\"total_cost\"] = (\n",
    "    gpt_4o_mini_result[\"input_tokens\"] * costs[\"gpt-4o-mini\"][\"input\"]\n",
    "    + gpt_4o_mini_result[\"output_tokens\"] * costs[\"gpt-4o-mini\"][\"output\"]\n",
    ") * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8564ff4-22d4-42b3-92f3-9b811d7e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_result[\"total_cost\"] = (\n",
    "    gemini_flash_result[\"input_tokens\"] * costs[\"gemini-1.5-flash\"][\"input\"]\n",
    "    + gemini_flash_result[\"output_tokens\"] * costs[\"gemini-1.5-flash\"][\"output\"]\n",
    ") * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27390800-f68d-4a8e-991c-c6582eeee6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_basic_analysis_cost_latency(gemini_flash_result, \"gemini-1.5-flash\")\n",
    "plt.savefig(\"task_cost_gemini-1.5-flash.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541563a-ff5e-4947-a604-e9eba18ce44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_basic_analysis_cost_latency(gpt_4o_mini_result, \"gpt-4o-mini\")\n",
    "plt.savefig(\"task_cost_gpt_4o_mini.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fb18e-6207-498d-957b-d90a9b5b8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_result.to_csv(\n",
    "    \"evaluation_docstrings/datasets/gpt_4o_mini_test_result.csv\", index=False\n",
    ")\n",
    "gemini_flash_result.to_csv(\n",
    "    \"evaluation_docstrings/datasets/gemini_flash_test_result.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24019e-7741-49e2-8404-9053b16f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_eval_df = evaluation_loop(\n",
    "    claude_caller,\n",
    "    gpt_4o_mini_result,\n",
    "    ground_truth=None,\n",
    "    model_name=\"claude-3-5-sonnet-latest\",\n",
    ")\n",
    "gpt_4o_mini_eval_df.to_csv(\n",
    "    \"evaluation_docstrings/datasets/gpt_4o_mini_task_evaluation_result.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c11dd-b821-46ec-b344-69922bf2fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_eval_df = evaluation_loop(\n",
    "    claude_caller,\n",
    "    gemini_flash_result,\n",
    "    ground_truth=None,\n",
    "    model_name=\"claude-3-5-sonnet-latest\",\n",
    ")\n",
    "gemini_eval_df.to_csv(\n",
    "    \"evaluation_docstrings/datasets/gemini-1.5_task_evaluation_result.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5eb74-a292-4730-ab82-d5220ecdcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = gpt_4o_mini_eval_df[\n",
    "    [\n",
    "        \"fields_correct\",\n",
    "        \"word_count_correct\",\n",
    "        \"line_numbers_correct\",\n",
    "        \"accuracy_score\",\n",
    "        \"coverage_score\",\n",
    "        \"clarity_score\",\n",
    "    ]\n",
    "]\n",
    "tmp[\"final_score\"] = (\n",
    "    (\n",
    "        tmp[\n",
    "            [\n",
    "                \"fields_correct\",\n",
    "                \"word_count_correct\",\n",
    "                \"line_numbers_correct\",\n",
    "                \"accuracy_score\",\n",
    "                \"coverage_score\",\n",
    "                \"clarity_score\",\n",
    "            ]\n",
    "        ]\n",
    "        == 1\n",
    "    )\n",
    "    .all(axis=1)\n",
    "    .astype(int)\n",
    ")\n",
    "totals = tmp.sum().sort_values()\n",
    "totals = totals / len(gpt_4o_mini_eval_df)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = totals.plot(kind=\"barh\")\n",
    "for i, v in enumerate(totals):\n",
    "    ax.text(v, i, \" {:.1f}%\".format(v * 100), va=\"center\")\n",
    "plt.title(\"Percent Correct by Category: gpt 4o mini\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Total Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"score_by_category_gpt4o_mini.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67def1d1-8d31-4ea9-bf7d-711145bcc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = gemini_eval_df[\n",
    "    [\n",
    "        \"fields_correct\",\n",
    "        \"word_count_correct\",\n",
    "        \"line_numbers_correct\",\n",
    "        \"accuracy_score\",\n",
    "        \"coverage_score\",\n",
    "        \"clarity_score\",\n",
    "    ]\n",
    "]\n",
    "tmp[\"final_score\"] = (\n",
    "    (\n",
    "        tmp[\n",
    "            [\n",
    "                \"fields_correct\",\n",
    "                \"word_count_correct\",\n",
    "                \"line_numbers_correct\",\n",
    "                \"accuracy_score\",\n",
    "                \"coverage_score\",\n",
    "                \"clarity_score\",\n",
    "            ]\n",
    "        ]\n",
    "        == 1\n",
    "    )\n",
    "    .all(axis=1)\n",
    "    .astype(int)\n",
    ")\n",
    "totals = tmp.sum().sort_values()\n",
    "totals = totals / len(gemini_eval_df)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = totals.plot(kind=\"barh\")\n",
    "for i, v in enumerate(totals):\n",
    "    ax.text(v, i, \" {:.1f}%\".format(v * 100), va=\"center\")\n",
    "plt.title(\"Percent Correct by Category: Gemini 1.5 flash\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.xlabel(\"Percentage correct\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"score_by_category_gemini_flash.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
