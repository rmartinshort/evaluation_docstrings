{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93895b-1503-4743-8b01-b8d614d18657",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aded7d3-7e93-44d8-a798-dcfe3ab8f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from evaluation_docstrings.llm.utils import load_secrets\n",
    "from evaluation_docstrings.llm.config import costs\n",
    "from evaluation_docstrings.plotting.utils import (\n",
    "    generate_basic_analysis_cost_latency,\n",
    "    generate_scores_barplot_dataset,\n",
    "    generate_cohen_kappa_plot,\n",
    ")\n",
    "from evaluation_docstrings.prompting.evaluation_prompts import (\n",
    "    DocStringEvaluationPrompt,\n",
    "    DocStringEvaluation,\n",
    ")\n",
    "from evaluation_docstrings.llm.structured_output_callers import (\n",
    "    AnthropicStructuredOutputCaller,\n",
    ")\n",
    "from evaluation_docstrings.task.utils import (\n",
    "    format_result_for_evaluation,\n",
    "    assemble_raw_match_scores,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def01ff-5457-46d6-8048-9a39fd94156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets = load_secrets()\n",
    "MODEL = \"claude-3-5-sonnet-latest\"\n",
    "claude_caller = AnthropicStructuredOutputCaller(api_key=secrets[\"ANTHROPIC_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a4f8d-89af-4e56-808a-5c495e3be2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_ground_truth.to_csv(\n",
    "    \"evaluation_docstrings/datasets/alignment_ground_truth.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddf239-4b79-48d0-aab2-fb3feaff7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_dataset = pd.read_csv(\n",
    "    \"evaluation_docstrings/datasets/alignment_ground_truth.csv\"\n",
    ")\n",
    "ground_truth_only = ground_truth_dataset[\n",
    "    [\"code_id\", \"gt_coverage_score\", \"gt_clarity_score\", \"gt_accuracy_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b08c1c-f04b-40fd-b080-9ec994eb09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(df, model, ground_truth=None):\n",
    "    assert model in costs\n",
    "    evaluation_result = defaultdict(list)\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        code_id = row[\"code_id\"]\n",
    "        evaluation_message = format_result_for_evaluation(row)\n",
    "        print(f\"Evaluating response for {i} : code id {code_id}\")\n",
    "\n",
    "        t1 = time.time()\n",
    "        try:\n",
    "            if model in [\"claude-3-5-sonnet-latest\", \"claude-3-5-haiku-latest\"]:\n",
    "                evaluation_response = claude_caller.invoke(\n",
    "                    evaluation_message,\n",
    "                    DocStringEvaluationPrompt,\n",
    "                    DocStringEvaluation,\n",
    "                    input_model_name=model,\n",
    "                    temperature=0,\n",
    "                )\n",
    "                tokens = claude_caller.token_counter(evaluation_response)\n",
    "            elif model in [\"gpt-4o\", \"gpt-4o-mini\"]:\n",
    "                evaluation_response = open_ai_caller.invoke(\n",
    "                    evaluation_message,\n",
    "                    DocStringEvaluationPrompt,\n",
    "                    DocStringEvaluation,\n",
    "                    input_model_name=model,\n",
    "                    temperature=0,\n",
    "                )\n",
    "                tokens = open_ai_caller.token_counter(evaluation_response)\n",
    "            elif model in [\"gemini-1.5-flash\", \"gemini-1.5-pro\"]:\n",
    "                evaluation_response = gemini_caller.invoke(\n",
    "                    evaluation_message,\n",
    "                    DocStringEvaluationPrompt,\n",
    "                    DocStringEvaluation,\n",
    "                    temperature=0,\n",
    "                )\n",
    "                tokens = gemini_caller.token_counter(evaluation_response)\n",
    "            else:\n",
    "                raise ValueError(f\"Model name {model} not recognized\")\n",
    "\n",
    "            model_output = evaluation_response[\"model_dump\"]\n",
    "            input_tokens = tokens[\"input_tokens\"]\n",
    "            output_tokens = tokens[\"output_tokens\"]\n",
    "            total_cost = (\n",
    "                costs[model][\"input\"] * input_tokens\n",
    "                + costs[model][\"output\"] * output_tokens\n",
    "            ) / 1e6\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        t2 = time.time()\n",
    "\n",
    "        evaluation_result[\"code_id\"].append(code_id)\n",
    "        evaluation_result[\"code_type\"].append(row[\"code_type\"])\n",
    "        evaluation_result[\"code_name\"].append(row[\"code_name\"])\n",
    "        evaluation_result[\"input_code\"].append(row[\"input_code\"])\n",
    "        evaluation_result[\"expert_docstrings\"].append(row[\"expert_docstrings\"])\n",
    "        evaluation_result[\"eval_model_critique\"].append(model_output[\"critique\"])\n",
    "        evaluation_result[\"accuracy_score\"].append(int(model_output[\"accuracy\"]))\n",
    "        evaluation_result[\"coverage_score\"].append(int(model_output[\"coverage\"]))\n",
    "        evaluation_result[\"clarity_score\"].append(int(model_output[\"clarity\"]))\n",
    "        evaluation_result[\"process_time\"].append(t2 - t1)\n",
    "        evaluation_result[\"input_tokens\"].append(input_tokens)\n",
    "        evaluation_result[\"output_tokens\"].append(output_tokens)\n",
    "        evaluation_result[\"total_cost\"].append(total_cost)\n",
    "\n",
    "    evaluation_df = pd.DataFrame(evaluation_result)\n",
    "\n",
    "    # join on ground truth labels if we have them\n",
    "    if isinstance(ground_truth, pd.DataFrame):\n",
    "        evaluation_df = evaluation_df.merge(ground_truth, on=\"code_id\")\n",
    "\n",
    "    return evaluation_df\n",
    "\n",
    "\n",
    "# get data ready for barplot\n",
    "def assemble_scores_barplot(scores_df, id_cols):\n",
    "    melted_df = scores_df.melt(\n",
    "        id_vars=id_cols,\n",
    "        value_name=\"score\",\n",
    "        value_vars=[c for c in plot_df if c not in id_cols],\n",
    "        var_name=\"score_type\",\n",
    "    )\n",
    "    totals = (\n",
    "        melted_df.groupby([\"score_type\", \"code_type\"])\n",
    "        .agg({\"score\": \"sum\", \"code_name\": \"count\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"code_name\": \"total\"})\n",
    "    )\n",
    "    totals[\"fraction\"] = totals[\"score\"] / totals[\"total\"]\n",
    "    return totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9575a-9363-468c-aaba-85d2910e5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_alignment_result.to_csv(\n",
    "    \"evaluation_docstrings/datasets/claude_sonnet35_alignment_result.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdb443-d1e9-49f6-b467-c46b158b9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result = pd.read_csv(\n",
    "    \"evaluation_docstrings/datasets/claude_sonnet35_alignment_result.csv\"\n",
    ")\n",
    "generate_cohen_kappa_plot(evaluation_result, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449a956-a754-4eae-b174-5e6b980b7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = assemble_raw_match_scores(\n",
    "    evaluation_result,\n",
    "    id_col=\"code_name\",\n",
    "    type_col=\"code_type\",\n",
    "    scores_columns={\n",
    "        \"accuracy_score\": \"gt_accuracy_score\",\n",
    "        \"clarity_score\": \"gt_clarity_score\",\n",
    "        \"coverage_score\": \"gt_coverage_score\",\n",
    "    },\n",
    ")\n",
    "barplot_df = generate_scores_barplot_dataset(\n",
    "    plot_df, id_cols=[\"code_name\", \"code_type\"]\n",
    ")\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(8, 5))\n",
    "sns.barplot(data=barplot_df, x=\"score_type\", y=\"fraction\", ax=axis, hue=\"code_type\")\n",
    "axis.set_ylabel(\"Fraction matching\")\n",
    "axis.set_xlabel(\"Score type\")\n",
    "# Add an overall title for the figure\n",
    "fig.suptitle(f\"Fraction success for evaluation ({MODEL})\", fontsize=16)\n",
    "plt.savefig(f\"{MODEL}_success_fraction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bd1b9-c3f5-4c73-ab21-f7a96228c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_basic_analysis_cost_latency(evaluation_result, model_name=MODEL)\n",
    "plt.savefig(f\"{MODEL}_evaluation_costs.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
