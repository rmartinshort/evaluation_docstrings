# evaluation_docstrings

An example project showcasing experiments in task evaluation with LLMs. The task is to generate docstrings from Python code

## Notes

- ipython notebooks for the alignment and evaluation can be found in the task module 
- Prompts for the task and llm judge can be found in the prompting module 
- Plotting utils are in the plotting module
- Tools for calling the various LLMs are in the llm module
- Tools for running the evaluation checks and calculating scores can be found in the evaluation module
- All the data used in this project is in the datasets folder

See the associated medium article for more details
